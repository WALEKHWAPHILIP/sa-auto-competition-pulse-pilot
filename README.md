# SA Auto Competition Pulse (Pilot)

A **research-engineering pilot** that turns South African automotive news into a **monthly competition-event time series** and links it to **monthly new-vehicle sales (NAAMSA)**. The repo is designed so a **reader** can run the pipeline, inspect raw snapshots, verify transformations, and reproduce the final tables/figures.

---

## Research question

**Do competition-relevant news events predict or coincide with changes in monthly new-vehicle sales in South Africa?**

This pilot is **descriptive**: it builds clean, auditable datasets + baseline plots + a baseline lag regression. It does **not** claim causal identification.

---

## What to look at first (quick replication path)

Start with the dataset schema and quality snapshot in `docs/data_dictionary.md`, then open the main figure `reports/figures/sales_vs_event_intensity.png` to see monthly sales plotted against event intensity over time. For coverage and data integrity, inspect `reports/tables/coverage_audit.csv` (month coverage across news/events/NAAMSA/panel) and `reports/tables/missingness_audit.csv` (months where sales totals are missing in the merged panel). Finally, review `reports/tables/lag_model_baseline.csv` for the baseline lag specification (events at t and t−1 with month/year controls) generated from `data/processed/panel_monthly_events_sales.csv`.

---

## What this repo produces (current, reproducible outputs)

### Processed datasets (`data/processed/`)
These are generated by the pipeline scripts:

- `news_articles.csv`  
  Deduped, analysis-ready news corpus with stable IDs + hashes.
- `monthly_event_counts.csv`  
  Month-level counts by event type (wide format).
- `naamsa_monthly_sales_headlines.csv`  
  Month-level sales totals parsed from NAAMSA PDFs, with evidence windows and warnings.
- `panel_monthly_events_sales.csv`  
  Final merged month panel: event counts + total sales (with missingness flags).

### Audit + documentation (`docs/` and `reports/`)
Generated from *the actual current data on disk*:

- `docs/data_dictionary.md`  
  Auto-generated schema snapshot (rows, columns, dtypes, missingness, examples).
- `reports/tables/coverage_audit.csv`  
  Month coverage across news/events/NAAMSA/panel.
- `reports/tables/missingness_audit.csv`  
  Explicit list of missing sales months in the merged panel.
- `reports/figures/sales_vs_event_intensity.png`  
  Sales vs total event intensity time series.
- `reports/tables/lag_model_baseline.csv`  
  Baseline lag regression results (events at t and t-1 + month and year controls).

**All tables/figures in `reports/` are generated deterministically by scripts in `src/analysis/` (not hand-edited).**

---

## Data sources (current)

All source rules are defined in `configs/sources.yaml` and global pipeline behavior is defined in `configs/pipeline.yaml`.

**Current ingestion in this pilot:**
- **NAAMSA** press releases index → PDF downloads → rule-based extraction from PDFs.
- News sources configured in `configs/sources.yaml` (and scraped via `src/news/scrape_news.py`).

> Note: scraping is done with rate limiting + snapshotting. Raw artifacts are stored for auditability.

---

## Event ontology (v1)

Multi-label event types are defined in `configs/labeling_schema.yaml`:

- `ENTRY_EXIT`
- `LAUNCH_PRODUCT`
- `PRICE_ACTION`
- `RECALL_QUALITY`
- `CAPACITY_SUPPLY`
- `DEALER_NETWORK`
- `EV_POLICY_INFRA`

Event labeling is **rule-based** (transparent), producing deterministic month-level counts.

---

## Repository layout

- `configs/`  
  Pipeline configuration registry (`pipeline.yaml`, `sources.yaml`, `labeling_schema.yaml`)
- `data/raw/`  
  Immutable snapshots (HTML/PDF) + manifests/hashes (audit trail)
- `data/processed/`  
  Analysis-ready datasets (CSV)
- `docs/`  
  Governance + documentation (ethics, definition-of-done, reproducibility checklist, data dictionary)
- `reports/figures/`  
  Final plots saved by analysis scripts
- `reports/tables/`  
  Final tables saved by analysis scripts
- `src/`  
  Pipeline code (news, events, naamsa, analysis, utils)

---

## Quickstart (Windows CMD, deterministic)

### 0) Create & activate venv (example)

- `cd /d C:\apps\sa_motoring_pilot\sa-auto-competition-pulse`
- `python -m venv .venv`
- `.\.venv\Scripts\activate`
- `python -m pip install -U pip`
- `pip install -r requirements.txt`


### 1) Sanity check: compile all modules

- `python -m compileall src`

### 2) Run the pipeline (core datasets)

#### 2.1 Scrape + dedupe news → `news_articles.csv`

- `python -m src.news.scrape_news --pipeline configs\pipeline.yaml --sources configs\sources.yaml`

#### 2.2 Label competition events → monthly counts

- `python -m src.events.label_events --pipeline configs\pipeline.yaml --schema configs\labeling_schema.yaml`
- `python -m src.news.count_labels --pipeline configs\pipeline.yaml`

#### 2.3 NAAMSA PDFs → monthly totals with evidence

- `python -m src.naamsa.discover_links --pipeline configs\pipeline.yaml --sources configs\sources.yaml`
- `python -m src.naamsa.download_pdfs --pipeline configs\pipeline.yaml`
- `python -m src.naamsa.parse_sales_headlines --pipeline configs\pipeline.yaml`

> After these steps, the repo should contain the processed CSVs listed in “What this repo produces”.

### 3) Build audit + documentation artifacts (ship-ready outputs)

#### 3.1 Coverage + missingness audits

- `python -m src.analysis.build_audit_tables`

#### 3.2 Auto-generate the data dictionary from current files

- `python -m src.analysis.generate_data_dictionary`

#### 3.3 Plot sales vs event intensity

- `python -m src.analysis.plot_sales_vs_events`

#### 3.4 Baseline lag model (events_t and events_t-1)

- `python -m src.analysis.run_lag_model_baseline`


### 4) One-command runners (recommended)

- `run_all.cmd` — rebuild pipeline + analysis outputs (uses existing local PDFs; scraping may take long if enabled in config).
- `run_tests.cmd` — compile + unit tests + deterministic QA gate (offline).

---

## How to verify outputs quickly (no guessing)

### Confirm processed datasets exist

- `dir data\processed`

### Confirm generated outputs exist

- `dir reports\tables`
- `dir reports\figures`
- `dir docs`


### Inspect the missingness audit

- `type reports\tables\missingness_audit.csv`

---

## Governance & reproducibility

This pilot treats governance as first-class:

* `docs/data_ethics.md` — data collection and ethics policy
* `docs/definition_of_done.md` — what “done” means for this pilot (ship checklist)
* `docs/reproducibility_checklist.md` — environment + rerun checklist
* `docs/data_dictionary.md` — auto-generated schema snapshot from current datasets

`docs/data_ethics.md` also summarizes collection constraints (robots/terms, minimization, and copyright-aware handling of text).

---

## Reuse notes (code vs data)

* **Code** is intended to be reusable under the repository’s open-source license.
* **Derived datasets** in `data/processed/` (event counts, monthly panels, audit tables) are produced by this pipeline and are intended to be reusable for research and teaching.
* **Raw snapshots** in `data/raw/` are stored for auditability and reproducibility. Any downstream redistribution or reuse should respect each publisher’s terms and robots/access policies (see `docs/data_ethics.md`).

---

## Method notes (what’s “research-grade” here)

This repo is intentionally built like an RA pipeline:

* **Raw snapshots are retained** (auditability).
* **Processed tables are reproducible** by rerunning scripts from configs.
* **Rule-based extraction stores evidence windows** (NAAMSA parsing transparency).
* **Event labeling is explicit and inspectable** (ontology + count outputs).
* **Analysis outputs are generated from the merged panel**, not hand-edited.

---

## Limitations (pilot scope)

* Month-level aggregation is coarse.
* News coverage is incomplete and non-random (selection bias).
* Rule-based labels can miss nuance (a baseline, not the final word).
* The baseline regression is descriptive (not causal).

---

## Next steps (research roadmap)

1. Expand news coverage across years (more sources, RSS where available).
2. Add a small gold-set for event labeling evaluation (precision/recall).
3. Add robustness specs (alternative lags, alternative intensity definitions).
4. Extend beyond total sales to segment/brand/model panels (future milestone).

---

## Regenerating “generated” files

These files are generated from current `data/processed/`:

* `docs/data_dictionary.md`
* `reports/tables/coverage_audit.csv`
* `reports/tables/missingness_audit.csv`
* `reports/figures/sales_vs_event_intensity.png`
* `reports/tables/lag_model_baseline.csv`

Regenerate with:


- `python -m src.analysis.build_audit_tables`
- `python -m src.analysis.generate_data_dictionary`
- `python -m src.analysis.plot_sales_vs_events`
- `python -m src.analysis.run_lag_model_baseline`

---

## How to cite

If you use or build on this repository, please cite it as:

Walekhwa, P. L. T. (2026). *SA Auto Competition Pulse (Pilot): News-derived competition events and NAAMSA sales (South Africa)*. GitHub repository.

---

## License & reuse

This project uses publicly available sources and stores snapshots for research reproducibility and auditability.

* **Code**: reusable under the open-source license included in this repository (see `LICENSE`).
* **Data**: downstream reuse should respect each publisher’s terms and access policies. If any source requests removal or changes access policy, update `configs/sources.yaml` and remove that source from the pipeline (see `docs/data_ethics.md`).

